{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "file = 't10k-images-idx3-ubyte'\n",
    "arr = idx2numpy.convert_from_file(file)\n",
    "'''\n",
    "for i in range(9):  \n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(arr[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    '''\n",
    "print(np.shape(arr[4]))\n",
    "\n",
    "def vertical_half(image):\n",
    "    h,w = image.shape\n",
    "    w_cutoff = w // 2\n",
    "    s1 = image[:,:w_cutoff]\n",
    "    s2 = image[:,w_cutoff:]\n",
    "    return s1, s2\n",
    "\n",
    "half_images = []\n",
    "for i in range(len(arr)):\n",
    "    img1,img2 = vertical_half(arr[i])\n",
    "    half_images.append(img1)\n",
    "    half_images.append(img2)\n",
    "    #plt.figure()\n",
    "    #plt.subplot(121)\n",
    "    #plt.imshow(img1)\n",
    "    #plt.subplot(122)\n",
    "    #plt.imshow(img2)\n",
    "    #plt.subplot(121)\n",
    "    #plt.imshow(half_images[i])\n",
    "    #plt.subplot(122)\n",
    "    #plt.imshow(half_images[i+1])\n",
    "\n",
    "\n",
    "half_imarr = np.asarray(half_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coastal-pastor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f84a9c0bf50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(restore(half_imarr[1000], half_imarr[5001]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entitled-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "casual-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnisthalfDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            tensor_image = self.transform(image)\n",
    "\n",
    "        return tensor_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "discrete-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.img_size = img_size\n",
    "        self.conv1 = nn.Conv2d(1, 28, (3, 3), padding=(1, 1))\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(28, 56, (3, 3), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(56, 112, (3, 3), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Downscale the image with conv maxpool etc.\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.deconv1 = nn.ConvTranspose2d(112, 56, (3, 3), stride=(2, 2))\n",
    "        # self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv2 = nn.ConvTranspose2d(56, 28, (2, 3), stride=(2, 2))\n",
    "#         self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.deconv3 = nn.ConvTranspose2d(28, 1, (2, 2), stride=(2, 2))\n",
    "        #self.upsamp1 = nn.UpsamplingBilinear2d(2)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.relu1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.deconv2(x)\n",
    "        x = self.relu2(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.deconv3(x)\n",
    "        x = self.relu3(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "medium-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 28  # The images are already resized here\n",
    "IMG_WIDTH = 14  # The images are already resized here\n",
    "\n",
    "SEED = 42\n",
    "TRAIN_RATIO = 0.75\n",
    "VAL_RATIO = 1 - TRAIN_RATIO\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "TRAIN_BATCH_SIZE = 32  # Let's see, I don't have GPU, Google Colab is best hope\n",
    "TEST_BATCH_SIZE = 32  # Let's see, I don't have GPU, Google Colab is best hope\n",
    "FULL_BATCH_SIZE = 32\n",
    "\n",
    "AUTOENCODER_MODEL_PATH = \"baseline_autoencoder.pt\"\n",
    "ENCODER_MODEL_PATH = \"baseline_encoder.pt\"\n",
    "DECODER_MODEL_PATH = \"baseline_decoder.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abroad-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"train_step\", \"val_step\", \"create_embedding\"]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):\n",
    "    # device = \"cuda\"\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    # print(device)\n",
    "\n",
    "    for batch_idx, train_img in enumerate(train_loader):\n",
    "        train_img = train_img.to(device)\n",
    "        #target_img = target_img.to(device)\n",
    "        #print(train_img.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc_output = encoder(train_img)\n",
    "        #print('encoder',enc_output.shape)\n",
    "        dec_output = decoder(enc_output)\n",
    "        #print('decoder',dec_output.shape)\n",
    "        \n",
    "        loss = loss_fn(train_img, dec_output)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def val_step(encoder, decoder, val_loader, loss_fn, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, train_img in enumerate(val_loader):\n",
    "            train_img = train_img.to(device)\n",
    "            #target_img = target_img.to(device)\n",
    "\n",
    "            enc_output = encoder(train_img)\n",
    "            dec_output = decoder(enc_output)\n",
    "\n",
    "            loss = loss_fn(train_img,dec_output)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "visible-content",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Creating Dataset ------------\n",
      "------------ Dataset Created ------------\n",
      "------------ Creating DataLoader ------------\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "\n",
    "transforms = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "print(\"------------ Creating Dataset ------------\")\n",
    "full_dataset = MnisthalfDataset(half_imarr, transforms)\n",
    "\n",
    "train_size = int(TRAIN_RATIO * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(\"------------ Dataset Created ------------\")\n",
    "print(\"------------ Creating DataLoader ------------\")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "full_loader = torch.utils.data.DataLoader(\n",
    "    full_dataset, batch_size=FULL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "light-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Dataloader Cretead ------------\n",
      "Moving models to CPU\n",
      "------------ Training started ------------\n",
      "Epochs = 0, Training Loss : 0.9239951372146606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:18<02:44, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 0, Validation Loss : 0.9251199960708618\n",
      "Epochs = 1, Training Loss : 0.9230300784111023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:36<02:24, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 1, Validation Loss : 0.9251199960708618\n",
      "Epochs = 2, Training Loss : 0.9280116558074951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:54<02:05, 18.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 2, Validation Loss : 0.9251199960708618\n",
      "Epochs = 3, Training Loss : 0.9266093969345093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [01:12<01:48, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 3, Validation Loss : 0.9251199960708618\n",
      "Epochs = 4, Training Loss : 0.9313117861747742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:30<01:31, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 4, Validation Loss : 0.9251199960708618\n",
      "Epochs = 5, Training Loss : 0.9160170555114746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:48<01:12, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 5, Validation Loss : 0.9251199960708618\n",
      "Epochs = 6, Training Loss : 0.9283717274665833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [02:06<00:54, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 6, Validation Loss : 0.9251199960708618\n",
      "Epochs = 7, Training Loss : 0.9300898313522339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [02:24<00:36, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 7, Validation Loss : 0.9251199960708618\n",
      "Epochs = 8, Training Loss : 0.9214169383049011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [02:42<00:18, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 8, Validation Loss : 0.9251199960708618\n",
      "Epochs = 9, Training Loss : 0.9186469912528992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:00<00:00, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased, saving new best model\n",
      "Epochs = 9, Validation Loss : 0.9251199960708618\n",
      "Training Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------ Dataloader Cretead ------------\")\n",
    "\n",
    "# print(train_loader)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "encoder = ConvEncoder()\n",
    "\n",
    "decoder = ConvDecoder()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Availaible moving models to GPU\")\n",
    "else:\n",
    "    print(\"Moving models to CPU\")\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# print(device)\n",
    "\n",
    "autoencoder_params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = optim.AdamW(autoencoder_params, lr=LEARNING_RATE)\n",
    "\n",
    "# early_stopper = utils.EarlyStopping(patience=5, verbose=True, path=)\n",
    "max_loss = 9999\n",
    "\n",
    "print(\"------------ Training started ------------\")\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train_loss = train_step(\n",
    "        encoder, decoder, train_loader, loss_fn, optimizer, device=device\n",
    "    )\n",
    "    print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "    val_loss = val_step(\n",
    "        encoder, decoder, val_loader, loss_fn, device=device\n",
    "    )\n",
    "\n",
    "    # Simple Best Model saving\n",
    "    if val_loss < max_loss:\n",
    "        print(\"Validation Loss decreased, saving new best model\")\n",
    "        torch.save(encoder.state_dict(), ENCODER_MODEL_PATH)\n",
    "        torch.save(decoder.state_dict(), DECODER_MODEL_PATH)\n",
    "\n",
    "    print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "\n",
    "print(\"Training Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "false-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(encoder, full_loader, embedding_dim, device):\n",
    "    \"\"\"\n",
    "    Creates embedding using encoder from dataloader.\n",
    "    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder\n",
    "    full_loader: PyTorch dataloader, containing (images, images) over entire dataset.\n",
    "    embedding_dim: Tuple (c, h, w) Dimension of embedding = output of encoder dimesntions.\n",
    "    device: \"cuda\" or \"cpu\"\n",
    "    Returns: Embedding of size (num_images_in_loader + 1, c, h, w)\n",
    "    \"\"\"\n",
    "    # Set encoder to eval mode.\n",
    "    encoder.eval()\n",
    "    # Just a place holder for our 0th image embedding.\n",
    "    embedding = torch.randn(embedding_dim)\n",
    "    \n",
    "    # Again we do not compute loss here so. No gradients.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, train_img in enumerate(full_loader):\n",
    "            # We can compute this on GPU. be faster\n",
    "            train_img = train_img.to(device)\n",
    "            \n",
    "            # Get encoder outputs and move outputs to cpu\n",
    "            enc_output = encoder(train_img).cpu()\n",
    "            #print(enc_output.shape)\n",
    "            # Keep adding these outputs to embeddings.\n",
    "            embedding = torch.cat((embedding, enc_output), 0)\n",
    "    \n",
    "    # Return the embeddings\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prerequisite-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SHAPE = (32,112,3,1)\n",
    "# We need feature representations for complete dataset not just train and validation.\n",
    "# Hence we use full loader here.\n",
    "embedding = create_embedding(encoder, full_loader, EMBEDDING_SHAPE, device)\n",
    "\n",
    "# Convert embedding to numpy and save them\n",
    "numpy_embedding = embedding.cpu().detach().numpy()\n",
    "num_images = numpy_embedding.shape[0]\n",
    "\n",
    "# Save the embeddings for complete dataset, not just train\n",
    "flattened_embedding = numpy_embedding.reshape((num_images, -1))\n",
    "np.save(\"data_embedding.npy\", flattened_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "automatic-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similar_images(image, num_images, embedding, device):\n",
    "    \"\"\"\n",
    "    Given an image and number of similar images to search.\n",
    "    Returns the num_images closest neares images.\n",
    "    Args:\n",
    "    image: Image whose similar images are to be found.\n",
    "    num_images: Number of similar images to find.\n",
    "    embedding : A (num_images, embedding_dim) Embedding of images learnt from auto-encoder.\n",
    "    device : \"cuda\" or \"cpu\" device.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_tensor = T.ToTensor()(image)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_embedding = encoder(image_tensor).cpu().detach().numpy()\n",
    "        \n",
    "    flattened_embedding = image_embedding.reshape((image_embedding.shape[0], -1))\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
    "    knn.fit(embedding)\n",
    "\n",
    "    _, indices = knn.kneighbors(flattened_embedding)\n",
    "    indices_list = indices.tolist()\n",
    "    return indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "persistent-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_images(test,indices_list):\n",
    "    plt.figure()\n",
    "    plt.imshow(test)\n",
    "    plt.show()\n",
    "    indices = indices_list[0]\n",
    "    for index in indices:\n",
    "        img = half_imarr[index-1]\n",
    "        print(index)\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "spiritual-naples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15775\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1761\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15859\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17973\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "embedding = np.load('data_embedding.npy')\n",
    "\n",
    "test = half_imarr[1]\n",
    "indices_list = compute_similar_images(test, 4, embedding, device)\n",
    "\n",
    "\n",
    "plot_similar_images(test,indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "polish-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore(im1, im2):\n",
    "    h1,w1 = im1.shape\n",
    "    h2,w2 = im2.shape\n",
    "    img = np.zeros((28,28),dtype=np.uint8)\n",
    "    img[:,:w1] = im1 \n",
    "    img[:,w1:] = im2\n",
    "    #dst = Image.new('L',(w1 + w2, h2))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-astrology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
